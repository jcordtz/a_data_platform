![microsoft](images/microsoft.png)

# Eine Datenplattform - *"zukunftssicher"*

[![en](https://img.shields.io/badge/lang-en-blue.svg)](README.md)
[![dk](https://img.shields.io/badge/lang-dk-red.svg)](README-da.md)
[![de](https://img.shields.io/badge/lang-de-yellow.svg)](README-de.md)
[![main](https://img.shields.io/badge/main-document-green.svg)](README.md)

<div style="text-align: center"><img src="images/tiger.jpg" width="400" /></div>

## Einf√ºhrung

Die IT-Welt befindet sich in diesen Zeiten (2024-) in einem erheblichen Wandel, vor allem aufgrund der "ChatGPTs", die unter der √úberschrift Generative AI ‚Äì oder einfach GenAI ‚Äì eine breite 
Palette neuer Funktionen rund um Text, Audio, Bilder und sogar Videos bieten.   Daher ist es wichtiger denn je, sicherzustellen, dass die richtigen Daten mit der richtigen Qualit√§t neben solchen 
Diensten verwendet werden, um diese Funktionen voll auszusch√∂pfen.

Aber wie konnte man vor einem Jahr vorhersagen, was heute m√∂glich ist und was f√ºr eine gro√üe Ver√§nderung es tats√§chlich ist, und damit planen, dies unterst√ºtzen zu k√∂nnen? Die kurze Antwort 
lautet: Sie k√∂nnten es nicht.
Und welche neuen "Herausforderungen" morgen mit sich bringen. Wir wissen es wahrscheinlich wirklich nicht, wir wissen nur, dass es Ver√§nderungen geben wird, und sie werden wahrscheinlich eher 
fr√ºher als sp√§ter eintreten.

In diesem Dokument wird beschrieben, wie eine Datenplattform geschaffen werden kann, die in der Lage ist, "was auch immer kommen mag" zu bew√§ltigen und somit zumindest aus Datensicht zu 
erm√∂glichen, diese Chancen/Herausforderungen zu bew√§ltigen.

Das Dokument basiert nicht auf dem, was "Best Practice" ist, sondern ist als "was Sie beachten sollten", wenn Sie eine Datenplattform erstellen m√∂chten.

Dieses Dokument basiert auf den Erfahrungen verschiedener Microsoft-Kunden, die die Erstellung einer Datenplattform mithilfe von Clouddiensten f√ºr analytische Aufgaben optimieren wollten.

>[!Hinweis]
>Dieses Dokument ist st√§ndig *in Vorbereitung*, da wir st√§ndig neue Dinge lernen.
>Die n√§chsten Themen werden h√∂chstwahrscheinlich Governance/Compliance und GenAI sein, da wir mehr dar√ºber im Zusammenhang mit > den verschiedenen Implementierungen erfahren, die wir von 
>Datenplattformen im Einsatz haben.

>[!Hinweis]
>Das Dokument basiert haupts√§chlich auf Microsoft-Technologiekomponenten.

>[!Hinweis]
>Die W√∂rter "Daten" und "Datenset" wird verwendet, um sich auf jede Darstellung von Information (Text/Bilder/Ton/Zahlen...) zu beziehen.


## Inhalt

Neben dem Hauptthema der Erstellung einer Datenplattform und den direkten Disziplinen, die ber√ºcksichtigt werden sollten, werden auch andere Themen im Zusammenhang mit der Datenplattform behandelt. Dies sind die folgenden (in separaten Abschnitten):

|Sektion|Sektion|Section|Sektion|
|-------|-------|-------|-------|
|[Sicherheit](./Security/Security-de.md)|[Gesetzgebung](./Security/Legislation-de.md)|[Exit-Strategie](./Security/Exit-and-risc-strategies-de.md)|[Risikobewertung](./Security/Exit-and-risc-strategies-de.md)|
|[Daten-Modellierung](./DataModelling/DataModel-de.md)|[Daten-ermiteln](./Operations/establish-data-de.md)|[Project Rollen](./Operations/project-roles-de.md)|[Anwender Rollen](./Operations/user-roles-de.md)|
|[Operationen/Widerstandsf√§higkeit](./Operations/Operations-de.md)|[Data-Operationen](./DataOps/DataOps-de.md)|[Data Mesh](./DataOps/Data-mesh-de.md)|[Selbstbedienung](./DataOps/Self-service-de.md)|
|[Generative AI](./GenAI/GenAI-de.md)|[Cloud-Umgebungen](./DataOps/Cloud-env-de.md)|[Namensstandards](./DataOps/Naming-standards-de.md)|[Meta data Handhabung](./DataOps/Data-mesh-de.md)|
|[Daten Agenten](./GenAI/Agent-de.md)|[Datenmarketsplatz](./GenAI/Marketplace-de.md)|||


## Hintergrund

In diesem Dokument geht es darum, wof√ºr Daten verwendet werden k√∂nnen und wie Sie sicherstellen k√∂nnen, dass Sie immer auf jede neue Situation vorbereitet sind, in der 
Daten werden ben√∂tigt. Und dass dieser Ansatz auch auf eine konforme und geregelte Weise erfolgt, die Ihre Richtlinien und Richtlinien f√ºr die Datennutzung widerspiegelt.

Eine allgemeine Botschaft des Inhalts dieses Dokuments ist die F√§higkeit, Ihren Benutzern, die Daten anfordern, sagen zu k√∂nnen:

                   Wenn Data heute nicht verf√ºgbar ist, wird es morgen verf√ºgbar sein.

Werfen wir einen genaueren Blick auf die Nutzungsmuster der hier behandelten Daten.

Wie in Abbildung 1 gezeigt, haben wir in der Mitte ein "St√ºck Daten", d.h. alle Informationen, die wir *irgendwo* verwenden m√∂chten.

Dieses "St√ºck" Daten wird h√∂chstwahrscheinlich auf unterschiedliche Weise verwendet. Hier wird folgendes skizziert:

1. **Reporting** , bei dem wir die Daten √ºber eine Anwendung bereitstellen, die es mir erm√∂glicht, "etwas" aus den "Zahlen" herauszulesen. H√∂chstwahrscheinlich w√§re dies eine Art Diagramm.
2. **Analytics** In dieser Situation werden die Daten aktiver, da sie vom Endbenutzer verwendet werden, um weiter zu arbeiten und Antworten auf neue Fragen zu erhalten. Dies geschieht h√∂chstwahrscheinlich mit einem BI-Tool wie PowerBI.
3. **AI/ML/GenAI** hier werden die Daten f√ºr "Simulationen/Vorhersagen" verwendet. Und es wird programmiert, sowohl mit Code als auch mit No-Code/Low-Code-Tools. Die Programmierung erfolgt in den meisten F√§llen mit Python und die Tools, die Sie hier treffen w√ºrden, decken Produkte wie Visual Studio/Eclipse √ºber Azure AI Foundry bis hin zu CoPilot Studio ab. Und je mehr Sie No-Code-Tools verwenden, desto mehr √§ndert sich die Sprache von Python zu nat√ºrlicher Sprache ("ChatGPT-Gespr√§ch").
4. **Teilbar** Wenn Sie Daten haben, die Sie interessant finden, m√∂chten Sie diese h√∂chstwahrscheinlich teilen k√∂nnen. In den meisten F√§llen muss dies eine kontrollierte Operation sein, damit wir sicher sind, dass der empfangende Teil korrekt ist und die Politik widerspiegelt, die wir dar√ºber haben sollten.
5. **Zugriffskontrolle** ist die einfache Tatsache, dass wir kontrollieren m√ºssen, wer was sieht. Dies wird in diesem Dokument in den Themen *Daten* und *Datens√§tze* behandelt.
6. **Compliance/Governance** umfasst die M√∂glichkeit, zu dokumentieren, wer auf was zugreift, und sicherzustellen, dass dies innerhalb unserer definierten Richtlinien bleibt.
7. **Self-Service** ist das "Nirwana" des Datenumgangs. Das bedeutet, dass der Endbenutzer den Zugriff auf und - vieleicht - die
Erstellung neuer Datens√§tze automatisch von der Datenplattform abgewickelt wird.
8. **Quellen** beschreibt, wie und welche Daten in die Datenplattform gebracht werden. Siehe n√§chste Abbildung.

![Abbildung 1](images/german/Slide24.jpeg)

*Abbildung 1*

Wenn wir dar√ºber sprechen, woher Daten kommen und wo sie verwendet werden, dann wird es so sein, dass (h√∂chstwahrscheinlich) viele Anwendungen, die Sie haben, bereits eine Art Berichts-/Analyseteil enthalten.
In diesem Fall lohnt es sich sehr, eine Entscheidung dar√ºber zu treffen, wie Sie dies f√ºr Ihre Zwecke richtig einsetzen k√∂nnen.

! [Abbildung 2](images/german/Slide25.jpeg)

*Abbildung 2*

Die Themen, die diskutiert werden m√ºssten, w√§ren dann (evtl. pro Anwendung):

1. Die Berichtsfunktionen werden dem Endbenutzer direkt von der Anwendung zur Verf√ºgung gestellt - m√∂glicherweise √ºber eine
gemeinsame "Schnittstelle", wie in der Abbildung gezeigt.
2. Alle Daten werden in die Datenplattform geschoben/gezogen und von hier aus offengelegt.
3. Nur Daten, die mit anderen Anwendungsdaten kombiniert werden m√ºssen, werden in die Datenplattform gepusht/gezogen.

Jede Kombination davon ist "richtig", wichtig ist, dass dokumentiert ist, was zu tun ist.

>[! HINWEIS]
>Es ist erforderlich, dass 2 und 3 funktionieren, dass Sie sichergestellt haben, dass Sie problemlos und ohne weitere Kosten (Lizenzen) 
>Ihre Daten aus diesen Anwendungen herausholen k√∂nnen und dass es daf√ºr eine dokumentierte und gepflegte Schnittstelle gibt, die auf
>einem Standard wie REST API, SQL oder Python SDK basiert ist.

## Die Vision
Die Vision der in diesem Dokument beschriebenen Datenplattform ist eine Plattform, bei der **Governance** und **Compliance** die Haupttreiber sind.

Daher ist das Konzept eines in sich konsistenten Datensatzes ein Schl√ºsselelement in diesem platform.

Stellen Sie auch eine Plattform bereit, die alles abdeckt, was kommt, und eine Situation schafft, in der Sie sagen k√∂nnen: ‚ÄûWenn es heute nicht auf der Plattform ist, wird es morgen da sein.‚Äú

Allerdings kann die beschriebene Datenplattform in jedem Szenario eingesetzt und an den tats√§chlichen Anwendungsfall angepasst werden muss.

## Gesamtparadigma

Um sicherzustellen, dass eine Datenplattform "was auch immer kommen mag" unterst√ºtzen kann, ist es wichtig, dass wir einige Richtlinien festlegen, was gelten sollte.

Dieses √ºbergeordnete Paradigma soll sicherstellen, dass eine Datenplattform die folgenden Merkmale erf√ºllt.

1. **Konsistenz** - Das L√∂sungsdesign ist konsistent, um die Bedienung und Weiterentwicklung von L√∂sungen zu vereinfachen. Der Konsistenzgrad eines Datenobjekts ist immer klar. Damit soll sichergestellt werden, dass der Wert der Daten immer kommuniziert werden kann.

2. **Datenkapselung** - Auf die Daten in der Dateninfrastruktur kann nur √ºber eine Schnittstelle zugegriffen werden, die steuert, wer wann Zugriff auf was hat. Die Schnittstelle sollte es Ihnen erm√∂glichen, die Dateninfrastruktur zu modifizieren, ohne externe Systeme zu beeintr√§chtigen.

3. **Moduliert** - Die L√∂sungen in der Datenplattform sollten als Module mit einer klar definierten Schnittstelle aufgebaut sein, damit es einfach ist, Ressourcen und Dienste zu ersetzen, hinzuzuf√ºgen oder zu entfernen.

4. **Technologieunabh√§ngig** - Die Architektur h√§ngt nicht von der verwendeten Technologie ab. Das bedeutet, dass die Prozesse, Funktionen und Schichten unabh√§ngig von der verwendeten Technologie gleich bleiben.

5. **Skalierbarkeit** - Skalierbarkeit (horizontal/vertikal) ist von Anfang an Teil des L√∂sungsdesigns, sodass Implementierung und Betrieb nicht durch Engp√§sse, Ausfallzeiten oder unerwartete Lizenzk√§ufe beeintr√§chtigt werden.

6. **Muss wiederherstellbar sein** - Alle in der L√∂sung verwendeten Dienste m√ºssen angehalten/gestoppt und sogar gel√∂scht werden k√∂nnen. Und daher auch gestartet/neu erstellt werden k√∂nnen. Und das ohne Datenverlust, √Ñnderung der Funktionalit√§t und des Zugriffs (APIs).

7. **Nachvollziehbarkeit** - Alle in der L√∂sung verwendeten Dienste m√ºssen f√ºr die Nutzung einzeln nachvollziehbar sein - sowohl aus Sicherheits- als auch aus Kostengr√ºnden.  Agilit√§t - Der Fokus muss auf einem Ansatz liegen, der auf Minimal Viable Product (MVP) basiert, sowie auf kontinuierlichem Feedback zu fr√ºheren Schritten im Datenfluss.

8. **Sicherheit** ‚Äì Sicherheit muss in die Gesamtarchitektur und spezifische L√∂sungsdesigns integriert werden, sowohl f√ºr die Informationssicherheit als auch f√ºr den Datenschutz. Der Austausch von Komponenten darf die Sicherheitsaspekte nicht beeintr√§chtigen. Compliance und Governance m√ºssen im Laufe der Zeit √ºber die verschiedenen Ebenen hinweg aufrechterhalten werden.

9. **Recycling** - L√∂sungen m√ºssen f√ºr das Recycling konzipiert sein. Die Architektur sollte Vorlagen f√ºr das L√∂sungsdesign enthalten, die die Markteinf√ºhrung beschleunigen und die Standardisierung gew√§hrleisten.

10. **Feedback** - Die Architektur muss basierend auf dem Feedback aus der Nutzung der Datenplattform kontinuierlich angepasst und verbessert werden.

## *Daten* und *Datensatz*

Die Konzepte *data* und *dataset* sind die "Kernkomponenten" der Datenplattform.

Ein sehr wichtiger Aspekt insbesondere von *Daten*, aber auch bis zu einem gewissen Grad *Datens√§tze* ist, dass ein gegebenes Objekt in 
der Lage sein muss, von sich selbst gehandhabt, gepflegt und gesichert zu werden, d.h. nicht durch eine Technologiekomponente wie eine 
Datenbank, daher ist die **file-handhabung** der Gesamtaspekt davon.

Wenn der Begriff *data* verwendet wird, bezieht er sich auf ein einzelnes Datenobjekt, z. B. eine Tabelle oder Datei, die nur die Daten 
eines bestimmten Objekts enth√§lt. Dies kann z. B. eine Quelltabelle wie Debitorenbuchhaltung oder Rechnungen sein.

Wenn hingegen der Begriff *Datensatz* verwendet wird, bedeutet dies eine Sammlung von Tabellen oder Dateien, die miteinander verbunden 
sind. Das kann zum Beispiel ein Data Mart mit Kunden-, Produkt- und Zeitdimensionen sowie Verkaufszahlen sein, auch Sternkarte genannt.   
Der Datensatz ist die Schl√ºsselkomponente der Datenplattform und weist einige spezifische Merkmale auf:

- Der Datensatz ist in sich geschlossen, d. h. er ist nicht von anderen Quellen abh√§ngig. Es enth√§lt alle Daten, die zur Unterst√ºtzung der anstehenden Aufgabe ben√∂tigt werden.
- Nur die Spalten/Zeilen, die f√ºr die ausgef√ºhrte Aufgabe relevant sind, sind im *Datensatz* vorhanden.
- Ein *Datensatz* geh√∂rt zu einer Gruppe und nicht zu bestimmten Personen ‚Äì daher ist der Besitzer in einer Azure-Konfiguration eine Gruppe mit einer Entra ID.
- Einem *Datensatz* sollten zwei zus√§tzliche Gruppen zugeordnet sein, eine f√ºr die Erstellung des Inhalts und eine andere mit Lesezugriff auf Daten.
- Der Zugriff auf einen *Datensatz* wird gew√§hrt, indem Personen zu der/den Gruppe(n) hinzugef√ºgt werden, je nachdem, welche Aufgabe sie ausf√ºhren m√ºssen.
- Ein *Datensatz* ist nicht an eine bestimmte Technologie wie eine relationale Datenbank gebunden. Das Dataset wird als Dateien gespeichert, h√§ufig als durch Kommas getrennte Dateien (csv) oder Parquet-Dateien. 
- Ein *Datensatz* kann √ºber die Technologie bereitgestellt werden, die am besten zu der jeweiligen Aufgabe passt - das kann dann eine Beziehungsdatenbank sein.

## Cloud-Zugang

Die in diesem Dokument beschriebene Datenplattform basiert auf Cloud-Technologien.

Und dieser Ansatz f√ºr eine Datenplattform hat einige Funktionen, die nur mit einem solchen Cloud-Ansatz erreicht werden k√∂nnen.

![Abbildung 3](images/german/Slide1.JPG)

*Abbildung 3*

Wie in *Abbildung 3* dargestellt, bietet die Verwendung von Cloud-Technologien Zugriff auf verschiedene Arten von Diensten.

**Infrastructure as a Service (IaaS)** ‚Äì Dies erm√∂glicht die Erstellung verschiedener Arten von virtuellen Maschinen und die Installation der 
gesamten darauf ben√∂tigten Software. Dabei garantiert der Cloud-Anbieter den Service bis auf Betriebssystemebene.  Beim IaaS-Ansatz 
fokussieren wir uns darauf, welche Produkte wir einsetzen wollen und liefern so die passenden "Maschinen" daf√ºr.

**Platform-as-a-Service (PaaS)**: Dies gilt f√ºr Dienste wie Datenbanken. Wir m√ºssen uns keine Sorgen um die Infrastruktur hinter diesen 
Diensten machen. Der Cloud-Anbieter stellt alle notwendigen Komponenten hinter dem Service selbst sicher. Dazu geh√∂ren Updates, neue 
Versionen und Verf√ºgbarkeit. Im PaaS-Setup konzentrieren wir uns nur darauf, welche Funktionalit√§t wir ben√∂tigen, und nicht darauf, 
welches "Produkt" wir daf√ºr ben√∂tigen.

**Software as a Service (SaaS)** ‚Äì Ein SaaS-Service ist eine Komplettl√∂sung wie ein ERP- oder HR-System. Hier stellt der Cloud-Anbieter den 
Zugriff auf eine vollst√§ndige Suite von Anwendungen, Datenbanken und Infrastrukturen sicher, die f√ºr das Funktionieren des SaaS-Dienstes 
erforderlich sind.

Die diskutierte Datenplattform basiert "nur" auf PaaS- und/oder SaaS-Diensten und damit in sehr begrenztem Umfang auf IaaS. 

Im Kapitel "Beispiele f√ºr Implementierungen" werden verschiedene M√∂glichkeiten beschrieben, dies mit verschiedenen PaaS- oder 
SaaS-Diensten zu tun.

Ein weiterer wichtiger Aspekt der Cloud ist, dass "alles Software ist". Das bedeutet, dass beim Erstellen eines neuen Servers beispielsweise die verschiedenen Komponenten, die der Server verwendet - wie Festplatten, Netzwerkkarten usw. - durch Senden von Befehlen an die Cloud-Infrastruktur generiert werden. Wir k√∂nnen also Software verwenden, um diese Komponenten herzustellen.

Dies wird als Infrastructure as Code (IaC) bezeichnet. In der beschriebenen Datenplattform wird dies beispielsweise verwendet, um eine relationale Datenbank zu erstellen und dann einen bestimmten Datensatz mithilfe von Code in diese Datenbank zu laden.

Dieses Modell wirft auch eine Diskussion dar√ºber auf, f√ºr welche Elemente in der zugrunde liegenden Infrastruktur sowohl der Cloud-Anbieter als auch der Kunde in den verschiedenen "Silos" von On-Prem, IaaS, PaaS und SaaS verantwortlich sind. Dies wird in diesem [Abschnitt](DataOps/Cloud-env.md) n√§her erl√§utert.

Die Vorg√§nge dieses Modells - bekannt als Datenoperationen oder DataOps - werden in diesem Abschnitt [Abschnitt](DataOps/DataOps.md) n√§her erl√§utert.


## Logische Architektur

Die Datenplattform ordnet *Daten* und *Datens√§tze* in verschiedenen Regionen gem√§√ü einer logischen Architektur an, wie in *Abbildung* 2 dargestellt. 

Damit soll sichergestellt werden, dass wir uns an die "Regeln" des Paradigmas halten k√∂nnen, wie bereits erw√§hnt.

![Abbildung 4](images/german/Slide2.JPG)

*Abbildung 4*

Die Bereiche repr√§sentieren verschiedene Zust√§nde der Reise von Daten zu Datens√§tzen und damit zu Berichten und Analysen.
In Bezug auf *Abbildung 4* k√∂nnen die verschiedenen Bereiche wie folgt beschrieben werden:

**Quellsysteme** sind alle Systeme, aus denen Daten extrahiert (batch) oder von denen Daten gesendet (streaming) werden sollen.

Der **Aufnahmebereich** ist der Ort, an dem Daten aus den Quellsystemen abgelegt werden. Die Daten werden in ihrem urspr√ºnglichen Format gespeichert. Wenn es sich bei den Daten um "Tabellendaten" handelt, werden keine √Ñnderungen an Zeilen oder Spalten vorgenommen, nicht einmal am Datenformat selbst. Die Daten werden in Dateien gespeichert - in der Regel durch Kommas getrennt oder Parkett m√∂glicherweise in ihrem Bin√§rformat - die beispielsweise f√ºr Video, Bild oder Audio gelten w√ºrden. Der *Aufnahmebereich* verf√ºgt √ºber eine Ordnerstruktur, die die Identifizierung der Datenquelle erleichtert. Es gibt keine Aktualisierung oder √úberschreibung bestehender Dateien - was bedeutet, dass bei einem neuen Laden neue Dateien erstellt werden. Ob es sich bei dieser Last um eine "Volllast" oder "√Ñnderungen seit dem letzten Mal" handelt, h√§ngt von den Bed√ºrfnissen und M√∂glichkeiten der jeweiligen Quellen ab.

Im Laufe der Zeit m√ºssen Dateien im *Aufnahmebereich* archiviert oder gel√∂scht werden, wenn dies gesetzlich vorgeschrieben ist (z. B. DSGVO).

Im Bereich **Transformieren** werden Daten aus dem Bereich *Aufnahmebereich* abgerufen und in ein "technisch nutzbares" Format ge√§ndert.

Eine Aufgabe besteht daher darin, Daten so zu transformieren, dass sie das gleiche Format haben ‚Äì zum Beispiel k√∂nnte es sein, die "schwierigen" Datentypen wie Datumsangaben (z. B. das Teilen der Uhrzeit vom Datum in eine separate Spalte) und Dezimalzahlen ("." oder ",." als Trennzeichen) zu "standardisieren".


Die einzelnen Datenelemente im Bereich *Transformieren* sind "unabh√§ngige Objekte". Das bedeutet, dass sie keinen Mix aus Daten aus einem oder mehreren Quellsystemen haben, nicht so verfeinert werden, dass die urspr√ºnglichen Informationen nicht vorhanden sind, und dass keine Begrenzung in der Anzahl der Zeilen oder Spalten vorgenommen wird. Sie k√∂nnen jedoch in Betracht ziehen, "fehlerhafte Zeilen" in ein separates Datenobjekt zu trennen.  

*Daten* im Bereich *Transformieren* existieren also als "reine" Objekte, mit denen man bequem arbeiten kann, wenn man sie zur Bildung von *Datens√§tzen* verwendet.

Eine weitere Aufgabe besteht darin, sicherzustellen, dass die verschiedenen Daten, die wir in diesem Bereich zur Verf√ºgung haben m√∂chten, einfach zusammengef√ºhrt werden k√∂nnen, was bedeutet, dass jedes Datenobjekt √ºber die "Referenzschl√ºssel" verf√ºgt, die erforderlich sind, um sich mit anderen Datenobjekten verbinden zu k√∂nnen.

Im Bereich **ver√∂ffentlichen** werden die *Datens√§tze* erstellt, die ben√∂tigt werden, um die verschiedenen Gesch√§ftsanforderungen zu erf√ºllen, die Daten von der Datenplattform ben√∂tigen. Hier werden Modelle wie Sternschemata verwendet, und die Bereitstellung dieser Datens√§tze erfolgt meist √ºber Data Marts (relationale Datenbanken).

Im Bereich **Verbrauchen** k√∂nnen die Endbenutzer der Datenplattform auf die ben√∂tigten Datens√§tze zugreifen und die Tools verwenden, die sie f√ºr am besten geeignet halten.

>[!Note]
>Das oben beschriebene Paradigma ist heute auch als *"Medaillon-Datenarchitektur"* bekannt, wobei *Bronze* der *Autnahmebereich*, *Silber* *Transformieren* und *Gold* *Ver√∂ffentlichen* ist. Die Medaillonarchitektur reflektiert nicht den *Verbrauchenbereich*. In diesem Dokument werden die Begriffe *Aufnahme*, *Transformieren*, *Ver√∂ffentlichen* und *Verbrauchen* verwendet, da dies die Verwendung bei den Kunden widerspiegelt, die die Inspiration f√ºr dieses Dokument sind.

### Datenplattform und das Data-Mesh-Paradigma

Data Mesh ist ein Architekturparadigma, das entwickelt wurde, um die Herausforderungen bei der Skalierung von Datenmanagement und -analyse in gro√üen, komplexen Organisationen zu bew√§ltigen. Es verlagert sich von zentralisierten Data Lakes und Warehouses zu einem dezentralen Ansatz, der es verschiedenen Teams erm√∂glicht, Daten unabh√§ngig voneinander zu verwalten und zu nutzen.

Dieses Thema im Zusammenhang mit der *Datenplattform* wird in diesem [Abschnitt](DataOps/Data-mesh-de.md) ausf√ºhrlicher behandelt

### Schnittstellen

*Abbildung 2* zeigt, dass die Schnittstelle zwischen den verschiedenen Bereichen genauso wichtig ist wie der Inhalt der Bereiche. Diese Schnittstellen m√ºssen die technologische Unabh√§ngigkeit gew√§hrleisten, die wir in der Plattform w√ºnschen ‚Äì es muss einfach sein, neue Dienste zu √§ndern/hinzuzuf√ºgen ‚Äì und sicherstellen, dass wir die Wege des Datenflusses kennen.

Die meisten Unternehmen/Institutionen bevorzugen die Verwendung einer oder mehrerer der folgenden Schnittstellenoptionen.  

**REST-API** - die "grundlegende" Schnittstelle, die die meisten (alle) neuen Dienste verwenden, um ihre Funktionalit√§t anzubieten. Diese Stufe ist hochtechnisch und eignet sich nicht f√ºr Low-Code/No-Code-Ans√§tze.

**SQL** ‚Äì die Standard-Datenabfragesprache, die von vielen verschiedenen Datenbanksystemen weit verbreitet ist und unterst√ºtzt wird. Viele kennen SQL von der Verwendung in relationalen Datenbanken, aber es ist auch in anderen Arten von Datenbanksystemen wie NoSQL-Datenbanken verf√ºgbar.

Wenn Sie in SQL "nur" die Funktionen verwenden, die Teil des SQL-Sprachstandards sind, bietet dies ein hohes Ma√ü an Flexibilit√§t. Das bedeutet, dass Sie keine spezifischen Funktionen verwenden, mit denen ein bestimmtes Datenbankprodukt eine SQL-Implementierung erweitert hat ‚Äì insbesondere die Verwendung der prozeduralen Sprachen, die beispielsweise in MS SQL Server oder Oracle DB zu finden sind.

**Python** ‚Äì die "neue" Datenverarbeitungssprache. Python ist weit verbreitet und spiegelt die aktuelle Art des Umgangs mit Daten wider. Python ist eine allgemeine Programmiersprache, die zur Laufzeit interpretiert wird.
Die Syntax von Python ist relativ einfach und daher leicht zu erlernen und bietet auch ein gutes Ma√ü an Lesbarkeit, was die Kosten f√ºr die Programmwartung senken sollte. Python unterst√ºtzt die Verwendung von Modulen und Paketen und f√∂rdert ein Modul zum Programmieren und Wiederverwenden von Code.

Die wirklich gute Unterst√ºtzung von Daten sowie eine umfangreiche Standardbibliothek machen Python im Kontext von Datenplattformen sehr beliebt.  Dar√ºber hinaus lieben Programmierer Python, weil sie das Gef√ºhl haben, dass es sie produktiver macht.

### Unterst√ºtzende Datendienste

Eine zus√§tzliche Komponente der Implementierung sind, wie Abbildung 2 zeigt, einige unterst√ºtzende Datendienste. Diesem sind weiter beschreibt in diesem [sektion](Supporting_Data_Services/SupportingDataServices-de.md)

### Data model

Das Thema des Umgangs mit der Modellierung von Daten in diesem Setup wird in diesem [section](DataModelling/DataModel-de.md) n√§her erl√§utert.

## Ein Sonderfall ‚Äì Echtzeit

Wir werden uns der Echtzeit-Datenverarbeitung als Einzelfall n√§hern und sie aus dieser Perspektive betrachten.

Real Time gibt es in verschiedenen Varianten, in diesem Dokument werden wir die folgenden Begriffe verwenden

- Echtzeit ‚Äì *Daten*, die sofort nach der Abholung geliefert werden.
- Nahezu in Echtzeit ‚Äì *Daten*, die aufgrund von Kommunikation oder Verarbeitung "verz√∂gert" werden.
- Dynamische Daten ‚Äì *Daten*, die aktualisiert werden und Aufmerksamkeit ben√∂tigen.

Die Datenplattformmethode in diesem Dokument ist **nicht** f√ºr die Echtzeitsituation geeignet, aber sie funktioniert gut f√ºr nahezu Echtzeit- und dynamische Daten. Dynamische Daten, die auf Ereignissen basieren, werden auf die gleiche Weise verarbeitet wie Near-Real-Time-Daten in der Datenplattform.

Der allgemeine Ansatz besteht darin, dass einige oder alle Echtzeit-*Daten* auch im *Aufnahmebereich* f√ºr eine weitere Verarbeitung gespeichert werden.
Dies erm√∂glicht es der Datenplattform, √ºber Funktionen zu verf√ºgen, die gegebenenfalls einen Echtzeitprozess unterst√ºtzen k√∂nnen, aber auch das gesamte Wissen zu verwalten, das im Laufe der Zeit aus den Echtzeitsituationen generiert werden kann.
Dies kann dann dazu beitragen, das Eintreten eines unerw√ºnschten Ereignisses ‚Äì wie z. B. eine Zugversp√§tung ‚Äì zu vermeiden.

Oder geben Sie detailliertere Informationen √ºber ein Torereignis in einem Fu√üballspiel und k√∂nnen Sie dann die Quoten auf der Grundlage der auf der Datenplattform verf√ºgbaren historischen Daten schnell √§ndern.

Um dies zu veranschaulichen, stellen Sie sich einen Zug vor, der Versp√§tung hat. Das System, das den Fahrg√§sten am Bahnhof die Informationen anzeigt, erh√§lt sofort die Echtzeitdaten √ºber die Versp√§tung und aktualisiert die Schilder entsprechend. Dabei werden die Daten selbst nur wenig verarbeitet.

![Abbildung 5](images/german/Slide3.JPG)

*Abbildung 5*

Diese *Daten* werden aber **auch** in der Datenplattform gespeichert, wo zwar etwas Zeit, aber nicht viel ist, um f√ºr die einzelnen Fahrg√§ste, die auf den versp√§teten Zug warten, einen Vorschlag f√ºr alternative Routen zu generieren. Diese Informationen k√∂nnen dann an eine App auf dem Smartphone gesendet werden. Dieser 

## Ein weiterer Sonderfall - ChatGPT/CoPilot

Da KI/ML/GenKI immer zug√§nglicher wird, werden die Anforderungen und damit Herausforderungen an die Datenelemente, die in solchen L√∂sungen verwendet werden, noch wichtiger.

Ein typischer Ansatz, um Ihre eigenen Daten in den Geltungsbereich einer GenAI-L√∂sung zu bringen, ist die Verwendung einer Methode namens RAG, die f√ºr Retrieval Augmented Generation steht.

RAG ist eine Architektur, die die F√§higkeiten eines Large Language Model (LLM) wie ChatGPT um ein Informationsabrufsystem erweitert, das **erding** Daten liefert. Durch das Hinzuf√ºgen eines 
Informationsabrufsystems haben Sie die Kontrolle √ºber die Erdungsdaten, die von einem LLM verwendet werden, wenn es eine Antwort formuliert. F√ºr eine Unternehmensl√∂sung bedeutet die 
RAG-Architektur, dass Sie generative KI auf Ihre Unternehmensinhalte beschr√§nken k√∂nnen, die aus **vektorisierten** Dokumenten und Bildern sowie anderen Datenformaten stammen, wenn Sie √ºber 
Einbettungsmodelle f√ºr diese Inhalte verf√ºgen.

![Abbildung 6](images/architecture-diagram.png)

*Abbildung 6*

Die Entscheidung, welches Informationsabrufsystem verwendet werden soll, ist entscheidend, da es die Eingaben f√ºr das LLM bestimmt. Das Informationsabrufsystem sollte Folgendes bieten:

1. Indizierungsstrategien, die alle Ihre Inhalte in gro√üem Umfang und in der von Ihnen gew√ºnschten H√§ufigkeit laden und aktualisieren.

1. Abfragefunktionen und Relevanzoptimierung. Das System sollte relevante Ergebnisse in den Kurzformaten zur√ºckgeben, die erforderlich sind, um die Anforderungen an die Tokenl√§nge von LLM-Eingaben zu erf√ºllen.

1. Sicherheit, globale Reichweite und Zuverl√§ssigkeit sowohl f√ºr Daten als auch f√ºr Abl√§ufe.

1. Integration mit Einbettungsmodellen f√ºr die Indizierung und Chat-Modellen oder Sprachverst√§ndnismodellen f√ºr den Abruf.

Azure AI Search ist ein Beispiel f√ºr eine solche "Datenbank", die Indizierungs- und Abfragefunktionen mit der Infrastruktur und Sicherheit der Azure-Cloud bietet.

Durch Code und andere Komponenten k√∂nnen Sie eine umfassende RAG-L√∂sung entwerfen, die alle Elemente f√ºr generative KI f√ºr Ihre propriet√§ren Inhalte enth√§lt.

Auf der Grundlage des oben Gesagten m√ºssen wir sicherstellen, dass die Daten, die wir den erstellten L√∂sungen zur Verf√ºgung stellen, genau mit dem √ºbereinstimmen, worauf der Benutzer Zugriff 
hat, so dass die Erdung und damit die Vektordatenbank nur dies enth√§lt. Dies kann √ºber den Ver√∂ffentlichungs-Layer erfolgen, da dieser Layer Datens√§tze mit dem genauen Inhalt darstellt. Die 
Aufgabe besteht also darin, die verwendeten Vektordatenbanken zu "laden" - und sicherzustellen, dass diese nach der Verwendung gestoppt/entfernt werden.

## Und ein dritter Special-Fall ‚Äì Agentic AI / Agents

Der generative AI-Ansatz hat sich zunehmend in Richtung agents entwickelt.

Ein Konzept, bei dem ein agent eine spezifische Aufgabe l√∂st und anschlie√üend seine Antwort an einen ‚Äû√ºbergeordneten‚Äú Service zur√ºckgibt, der dann die teilweisen Antworten der verschiedenen agents zu einer ‚Äûvollst√§ndigen‚Äú Antwort auf eine Anfrage kombiniert (unter Verwendung eines MCP-Servers).

Als Beispiel f√ºhre ich derzeit eine Diskussion mit einer NGO, die Hilfe leistet, wenn eine Katastrophe eintritt (Erdbeben, St√ºrme, Erdrutsche usw.). Keine Katastrophe ist wie die andere, daher h√§ngt die ben√∂tigte Hilfe von vielen Faktoren ab, wie z.‚ÄØB. Geografie, verf√ºgbaren Anbietern, Infrastruktur, Religion usw. Das bedeutet auch, dass sich der ben√∂tigte Prozess von Fall zu Fall unterscheidet.

Dies passt nicht gut in ein standardisiertes Framework. Daher m√ºssen sie die Dinge immer irgendwie zusammenstellen, und je nach Situation kann dies von relativ machbar bis hin zu sehr komplex reichen.

Was wir daher diskutieren, ist, ob ‚Äì und wie ‚Äì wir einen Ansatz verfolgen k√∂nnen, bei dem die relevanten tables in ihrer Systemlandschaft jeweils von einem agent fronted werden.

Jeder dieser agents beschreibt die F√§higkeiten der einzelnen table, z.‚ÄØB. was sie enth√§lt, wie sie genutzt werden kann und wie sie mit anderen Informationsteilen kombiniert werden kann usw.

Vor diesen agents befindet sich dann ein master agent, der f√ºr jeden einzelnen Fall konfiguriert (grounded) wird. Die relevanten Personen k√∂nnen diesen master agent nutzen, um per Prompt Antworten zu erhalten. Sie k√∂nnen auch eine Applikation erstellen, die zum jeweiligen Fall passt, und diese anschlie√üend den Helfern zur Verf√ºgung stellen.

In diesem Szenario w√ºrde jede einzelne table eines datasets in der data platform von einem agent fronted werden. Diese agents k√∂nnen dann je nach der jeweiligen Aufgabe in Aktion treten.

![Abbildung 6a](images/german/slide27.jpg)

*Abbildung 6a*

Ich habe selbst ein Setup f√ºr einen Kunden unter Verwendung von VIBE programming erstellt. Hier k√∂nnen Sie mehr √ºber VIBE im Microsoft‚ÄëKontext lesen.
(https://learn.microsoft.com/en-us/training/modules/introduction-vibe-coding/)

Ich habe dies verwendet und meine Idee wie folgt formuliert. Ich verwendete Englisch als Sprache (Arbeitsmerkmal üòÑ), aber man h√§tte das nat√ºrlich genauso gut auf Deutsch machen k√∂nnen (mit Copilot verwendet).

---
*I am participating in a creative Prompt to Prototype challenge where the end goal is to build a web-based prototype in 4 modules:*  
*1. Research.*  
*2. Product Requirements Document (PRD).*  
*3. Branding & Logo*.  
*4. Prototype*.  
*Here is my idea: I would like to create a setup where tables in an AzureSQL Database each are represented by an agent. Each agent provides information*  
*about what the content represent, what columns can be returned and which if these columns could be used to join with other tables also provided through*  
*agents. In this way each agent would be able to return the appropriate part of information needed to answer an overall task request. Notice that each agent*  
*must be able to connect to AzureSQL database.*  
*Please help me scope it by filling out the following details:*  
*1. Scenario (theme or context)*  
*2. Audience (target users)*  
*3. Vibe Words (2‚Äì3 adjectives for style and tone)*  
*4. Format (type of web experience ‚Äî app, game, dashboard, interactive toy, quiz etc.)*  
*5. Concept (short description)*  
*6. Goal (what it aims to achieve)*  
*7. Title (placeholder name)*  
*8. MVP Features (small, completable in <2 hours)*  
*9. Stretch Goals (2‚Äì3 enhancements)*  
*Output the result in a table with these columns: Scenario | Audience | Vibe Words | Format | Concept | Goal | Title | MVP Features | Stretch Goals.*  
  
---

Anschlie√üend habe ich Visual Studio Code verwendet, um mithilfe von GitHub Copilot eine Anwendung zu erstellen.

---
  
*Attached is my Product Requirements document named DataAgentHub-vs_PRD.md. Please refine and update this document to be optimized*  
*for vibe coding a lightweight client-side web application.*  
  
*Make the purpose clear, identify core features for a simple prototype, outline key user flows, and ensure it supports building with limited time.*  
  
*Update the document directly with your improvements.*  
  
---

Und habe diese Application erhalten.

![Vibe-application](images/VIBE-prog1.png)

## Umgebungen

In diesem Abschnitt wird erl√§utert, wie sechs Umgebungen zum Implementieren eines Datenplattformprojekts verwendet werden k√∂nnen. Je nach Vorlieben k√∂nnen Sie nat√ºrlich entscheiden, wie viele dieser Umgebungen Sie haben m√∂chten und wie Sie die beschriebenen Aufgaben auf weniger oder mehr Umgebungen verteilen.

1. Sandbox ‚Äì diese Umgebung wird f√ºr MVP-Tests verwendet.
2. Projektr√§ume - Entwicklungsumgebungen.
3. Entwicklung ‚Äì Code, der die Grundlage einer Produktionsumgebung bildet.
4. Test - Funktionstest.
5. Qualit√§tssicherung ‚Äì Code-Review.
6. Vorproduktion - Testen von Produktionsdaten.
7. Produktion - Produktionsdaten.

## Datenrichtlinien

Um die Datenrichtlinien zu steuern, die Sie verwenden m√∂chten, m√ºssen Sie sicherstellen, dass Sie die Art der Umgebung verstehen, die ein bestimmter Auftrag ausf√ºhrt. In diesem Dokument werden 5 Arten von Umgebungen in der Diskussion der Prozesse verwendet.

1. Sandbox - Umgebung, die zum Testen der Funktionalit√§t jedes Dienstes verwendet wird. Diese Umgebungen enthalten KEINE Gesch√§fts-/Firmendaten.
2. Projektr√§ume ‚Äì Diese Umgebungen werden verwendet, um Umgebungen einzurichten, die Tools und Daten enthalten, die zur Ausf√ºhrung einer Entwicklungsaufgabe verwendet werden.
3. Nicht-Produktion ‚Äì Umgebungen, die Entwicklungs-, Qualit√§tssicherungs- und Testszenarien enthalten.
4. Fertigung ‚Äì Umgebungen, die Vorproduktions- und Produktionsszenarien unterst√ºtzen.
5. Vertraulich ‚Äì Umgebungen, die die Verarbeitung streng vertraulicher Daten unterst√ºtzen.

## Daten und Sicherheit

Datensicherheit ist ein kritisches Element des Betriebs eines jeden Unternehmens. Es befasst sich mit dem Schutz von Daten vor unrechtm√§√üigem Zugriff, Verschlechterung oder Diebstahl √ºber die gesamte Lebensdauer der Daten. Mit der Einf√ºhrung robuster Datensicherheitsma√ünahmen k√∂nnen Unternehmen ihre kritischen Ressourcen sichern, Compliance erreichen und das Vertrauen der Kunden in den Umgang mit Daten erhalten.

Datensicherheit ist von entscheidender Bedeutung, da sie Unternehmen vor Cyberangriffen, Insider-Bedrohungen und menschlichem Versagen sch√ºtzt, die zu Datenschutzverletzungen f√ºhren k√∂nnen. Zu den wesentlichen Faktoren f√ºr die Datensicherheit geh√∂ren Vertraulichkeit, Integrit√§t, Verf√ºgbarkeit und Compliance. Angesichts wachsender Bedrohungen f√ºr Daten m√ºssen Unternehmen ihre Daten an der Quelle sch√ºtzen, um die Datensicherheit aufrechtzuerhalten und Daten aus einem Angriff schnell wiederherzustellen. Der Zweck der Datensicherheit besteht darin, Daten vor allen Formen des Missbrauchs, einschlie√ülich Cyberangriffen und menschlichem Versagen, zu sch√ºtzen.

Zusammenfassend l√§sst sich sagen, dass die Wahrung der Vertraulichkeit, Integrit√§t und Verf√ºgbarkeit der Informationen eines Unternehmens f√ºr die Datensicherheit unerl√§sslich ist. Es unterst√ºtzt den Schutz kritischer Ressourcen, hilft bei der Erf√ºllung der Compliance-Anforderungen bestimmter Standards und erh√§lt das Vertrauen der Kunden.

Im Abschnitt [Sicherheit/Security-de.md] finden Sie eine viel detailliertere Diskussion des Sicherheitsaspekts.
In diesem Abschnitt geht es weiter mit einigen weiteren technischen M√∂glichkeiten.

Abbildung 7 zeigt verschiedene Datenschutzmethoden, die in Azure verf√ºgbar sind. Allgemeine Themen wie Netzwerksicherheit oder Multi-Faktor-Authentifizierung werden jedoch nicht behandelt, da davon ausgegangen wird, dass diese bereits implementiert sind.

![Abbildung 7](images/german/Slide8.JPG)

*Abbildung 7*

**Anwendungsbasierte Zugriffskontrolle** - deckt die Tatsache ab, dass eine Anwendung wie SAP, Snowflake, Fabric, Dynamics usw. eine Anmeldung erfordert und somit den korrekten Zugriff auf die zugrunde liegenden Daten erm√∂glicht, die in der Anwendung verwendet werden. H√§ufig ist der zugrunde liegende Datenspeicher eine (relationale) Datenbank, auf die von der Anwendung aus √ºber ein Dienstkonto zugegriffen werden kann. 
Rollenbasierte Zugriffskontrolle ‚Äì auch bekannt als RBAC. Dies steuert den Zugriff auf eine bestimmte Ressource und wie sie verwendet werden kann. Also in popul√§ren Begriffen - k√∂nnen Sie auf das Speicherkonto zugreifen?

Die **Attributbasierte Zugriffskontrolle** ‚Äì auch bekannt als ABAC ‚Äì bietet oft einen zus√§tzlichen Mechanismus, um Zugriff zu gew√§hren, um eine "Suche" in einem anderen System durchzuf√ºhren. Beispielsweise kann man auf ein Speicherkonto zugreifen, aber es kann sich um einen Ordner handeln, f√ºr den man Teil eines bestimmten Projekts sein muss. In diesem Fall k√∂nnen Sie einen ABAC-"Lookup" durchf√ºhren, der dies √ºberpr√ºft, bevor Sie den Zugriff gew√§hren - abh√§ngig von diesem Ergebnis.

**Identit√§tsbasierte Zugriffskontrolle** ‚Äì umfasst die M√∂glichkeit, einer bestimmten Ressource eine Identit√§t zuzuweisen ("ein Mensch" zu werden). Und dann stellen Sie sicher, dass nur diesem "Menschen" Zugriff auf ein bestimmtes Speicherkonto gew√§hrt wird, und daher m√ºssen Sie diese Anwendung verwenden, um an die Daten zu gelangen.

**Verschl√ºsselungsbasierte Zugriffskontrolle** - Dies ist keine wirkliche Zugriffskontrolle, da der Datenspeicher verf√ºgbar ist, man die Daten jedoch nur lesen/verwenden kann, wenn man den Schl√ºssel zur Entschl√ºsselung hat. Es kann (sollte) also Teil Ihrer Verteidigung sein.

**L√∂schbasierte Zugriffssteuerung** ‚Äì dieser Ansatz ist nur im bereich *Ver√∂ffentlichen* verf√ºgbar. Dieser Ansatz nutzt den Aspekt des *Ver√∂ffentlihen* bereichs, dass ein *Datensetz* nur "so lange lebt, wie es genutzt wird", in diesem Fall "... richtig verwendet wird". Da dieser Ansatz die F√§higkeit erfordert, einen bestimmten Datenspeicher neu erstellen zu k√∂nnen, kann dies auch als Verteidigungsmechanismus verwendet werden. Wenn es also zu einem Angriff kommt, besteht der einfachste Weg, dies zu stoppen, darin, die Ressource w√§hrend des Angriffs einfach zu entfernen, wenn die Gefahr eines Datenverlusts besteht.

## Daten operationen

Laut Wikipedia ist DataOps eine Sammlung von Praktiken, Prozessen und Technologien, die eine ganzheitliche und prozessorientierte Sicht auf Daten mit Automatisierung und Methoden aus dem agilen Software-Engineering kombiniert, um Qualit√§t, Geschwindigkeit und Zusammenarbeit zu verbessern und eine Kultur der kontinuierlichen Verbesserung rund um die Datenanalyse zu f√∂rdern.

DataOps sind weiter beschreibt in diesem [sektion](DataOps/DataOps-de.md)


### Umgebungen und Tags

In den verschiedenen Umgebungen m√ºssen unterschiedliche Tags verwendet werden, um die Art der Umgebung zu identifizieren. Die folgende Tabelle enth√§lt Beispiele f√ºr Tags, die verschiedenen Umgebungen zugeordnet werden k√∂nnen.

|Umwelt/Dach|Sandkasten|Projektraum|Nicht-Produktion|Produktion|Vertraulich|Werte|
|---------|---------|----------|---------------|----------|---------|-------|
|Eigent√ºmer der Daten|Pr√ºfung|Erforderlich|Erforderlich|Erforderlich|Erforderlich|Name des Eigent√ºmers|
|Environment|Erforderlich|Erforderlich|Erforderlich|Erforderlich|Erforderlich|Die Art der Umgebung, bz. "Sandbox" |
|Kostenstelle|Erforderlich|Erforderlich|Erforderlich|Erforderlich|Erforderlich|Kostenstelle|

- √úberarbeitung ‚Äì Tag solte vorhanden sein.
- Erforderlich ‚Äì Tag muss vorhanden sein, andernfalls wird die Installation verweigert.
- Nicht zutreffend ‚Äì Nicht zutreffend.

## Entwicklungsumgebung - Projektraum

Eine M√∂glichkeit, eine sichere Entwicklungsumgebung einzurichten, k√∂nnte die Verwendung eines Projektraums sein.

Diese Projektr√§ume stellen eine isolierte Umgebung dar, die sich in der Regel im Besitz einer Gruppe befindet.

Im Projektraum werden Daten, Tools und Code vollst√§ndig isoliert erstellt/gepflegt. Der Zugriff auf einen Projektraum erfolgt durch Hinzuf√ºgen oder Abrufen von Personen aus den entsprechenden Gruppen.

Die folgende Abbildung 8 zeigt ein Beispiel f√ºr einen Projektbereich in der Datenplattformumgebung.

![Abbildung 8](images/german/Slide5.JPG) 

*Abbildung 8*

Entwicklungen, die in einem Projektraum stattfinden, k√∂nnen dann z.B. √ºber einen CI/CD-Prozess in die einheitliche Datenplattform "eingecheckt" werden. Ein Beispiel hierf√ºr finden Sie im Kapitel "CI/CD-Beispiel".

Alle Daten, die f√ºr die Durchf√ºhrung der Entwicklung ben√∂tigt werden, k√∂nnen/sollten einen Prozess durchlaufen, der sie zu "Nicht-Produktionsdaten/-datens√§tzen" macht.

Damit Daten/Datens√§tze in diesen Projektbereichen schreibgesch√ºtzt sind, muss der Besitz einer anderen, aber eindeutigen Gruppe zugewiesen werden.
In den seltenen Situationen, in denen eine Integrationsverbindung zwischen verschiedenen Projektr√§umen erforderlich ist, sollte das Eigentum in einer eigenen Gruppe platziert werden, die f√ºr diese Projektr√§ume noch einzigartig ist.


## CI/CD-Beispiel

Wie oben erw√§hnt, sollte die Verwendung von CI/CD-Prinzipien (Continuous Integration/Continuous Deployment) in Betracht gezogen werden, um sicherzustellen, dass die Codierung in der Datenplattform konsistent gehandhabt wird. 
Solche Prozesse verf√ºgen √ºber Pipelinestrukturen, die beschreiben, welche Prozesse Code durchl√§uft, wenn er in der Produktion bereitgestellt wird.

Abbildung 9 zeigt ein ‚Äì vereinfachtes ‚Äì Beispiel f√ºr einen solchen Workflow.

![Abbildung 9](images/german/Slide10.JPG) 

*Abbildung 9*

Im Zusammenhang mit der laufenden Entwicklung und dem Testen m√ºssen Sie h√§ufig in der Lage sein, Daten in Nicht-Produktionsumgebungen zu verarbeiten. Wahrscheinlich haben Sie keine Berechtigung oder m√∂chten Produktionsdaten in diesen Umgebungen verwenden. Zu Testzwecken k√∂nnen fehlerhafte Daten auch in Datens√§tze eingef√ºgt werden, um m√∂gliche Ausnahmeszenarien zu adressieren.

## Ein praktischer Ansatz

Basierend auf den Diskussionen in diesem Dokument zeigt *Abbildung 10*, wie dies im "wirklichen Leben" aussehen k√∂nnte. Links neben dieser Abbildung befinden sich die Quellsysteme, die "jemandem" geh√∂ren, der normalerweise als Systembesitzer bezeichnet wird. Diese Systembesitzer sind daf√ºr verantwortlich, dass die Datenplattform Zugriff auf die richtigen Systeme hat. In der Abbildung haben wir also 3 Systeme namens App 1, App 2 und App 3, und sie geh√∂ren jeweils einem Systembesitzer mit dem Namen Systembesitzer 1 bis 3. 

In der Mitte befindet sich die Datenplattform mit dem Bereich Ingest, Transform und Verbrauchen. Im Aufnahmebereich sehen Sie, dass Daten eins zu eins aus den verschiedenen Apps 1 bis 3 √ºbernommen werden. Dann haben wir einen Transformationsprozess, der diese Rohdaten in einen nutzbaren Zustand bringt. 
Auf der rechten Seite der Abbildung sehen Sie, was von den Endbenutzern im Ver√∂ffentlichungsbereich verlangt wird. Der erste Benutzer, Datenbenutzer 1 genannt, ben√∂tigt Daten, die nur aus App 1 stammen, sodass das erforderliche Dataset mit dem Namen Datenprodukt A ein unkomplizierter Prozess ist. 

Datenbenutzer 2 ben√∂tigt Daten, die sowohl aus App 1 als auch aus App 2 stammen, aber die in App 3 gefundenen Daten m√ºssen aus diesem Datensatz herausgefiltert werden, so dass der Prozess in diesem Fall etwas komplizierter ist, aber da der Transformationsbereich einen Bereich darstellt, in dem Daten leicht kombiniert (und auch ausgeschlossen) werden k√∂nnen, ist die Grundlage daf√ºr vorhanden.  Daher ist es recht einfach.

Gleiches gilt f√ºr Datenprodukt C, das Daten aus App 2 ohne Daten in App 3 darstellt.

![Abbildung 10](images/german/Slide7.JPG)

*Abbildung 10*

Dies zeigt auch, wie die Datenplattform in der Lage sein sollte, die Gesch√§ftsanforderungen schnell und nahtlos zu unterst√ºtzen.

Der Gesamtansatz **Wenn ein Datensatz heute nicht verf√ºgbar ist, wird er f√ºr morgen bereit sein** dann unterst√ºtz werden.

## Beispiele f√ºr Implementierungen

Im Folgenden finden Sie Beispiele f√ºr die Implementierung einer Datenplattform mit verschiedenen Diensten. Denken Sie daran, dass das √ºbergreifende Paradigma die technologische Unabh√§ngigkeit ist, daher sollte man "mischen und anpassen", was zu den eigenen Gesch√§ftsm√∂glichkeiten und Herausforderungen passt.

>[!Note]
>Diese Bereiche enthalten derzeit nur zus√§tzliche beschreibende Informationen, werden aber sp√§ter auch Beispiele f√ºr Infrastructure-as-Code enthalten.

[Azure-basiert](Microsoft/Azure/Azure-de.md) ‚Äì Verwendung von PaaS-Diensten von Azure

[Synapse-basiert](Microsoft/Synapse/Synapse-de.md) ‚Äì mit Synapse PaaS-Dienst

[Fabric-basiert](Microsoft/Fabric/Fabric-de.md) - unter Verwendung der SaaS-L√∂sung Microsoft Fabric

[DataBricks-basiert](Partners/Databricks/Databricks-de.md) ‚Äì unter Verwendung des 1st-Party-PaaS-Dienstes Azure DataBricks

[Snowflake-basiert](Partners/Snowflake/Snowflake-de.md) - Verwendung des SaaS-Dienstes Snowflake zusammen mit Azure-Diensten

[Microsoft Purview und die Datenplattform](Microsoft/Purview/Purview-de.md) ‚Äì Governance und Compliance der Datenplattform mit Purview


[![en](https://img.shields.io/badge/lang-en-blue.svg)](README.md)
[![dk](https://img.shields.io/badge/lang-dk-red.svg)](README-da.md)
[![de](https://img.shields.io/badge/lang-de-yellow.svg)](README-de.md)
[![main](https://img.shields.io/badge/main-document-green.svg)](../README.md)